{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Sources\" data-toc-modified-id=\"Sources-0.1\">Sources</a></span></li></ul></li><li><span><a href=\"#Data-Exploration\" data-toc-modified-id=\"Data-Exploration-1\">Data Exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gender-influence\" data-toc-modified-id=\"Gender-influence-1.1\">Gender influence</a></span></li></ul></li><li><span><a href=\"#Feature-engineering\" data-toc-modified-id=\"Feature-engineering-2\">Feature engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Titres\" data-toc-modified-id=\"Titres-2.1\">Titres</a></span></li><li><span><a href=\"#Remplir-les-valeurs-manquantes\" data-toc-modified-id=\"Remplir-les-valeurs-manquantes-2.2\">Remplir les valeurs manquantes</a></span></li><li><span><a href=\"#Traitement-des-noms-et-des-titres\" data-toc-modified-id=\"Traitement-des-noms-et-des-titres-2.3\">Traitement des noms et des titres</a></span></li><li><span><a href=\"#Les-catégories-de-prix\" data-toc-modified-id=\"Les-catégories-de-prix-2.4\">Les catégories de prix</a></span></li><li><span><a href=\"#Point-d'embarquement\" data-toc-modified-id=\"Point-d'embarquement-2.5\">Point d'embarquement</a></span></li><li><span><a href=\"#Cabines\" data-toc-modified-id=\"Cabines-2.6\">Cabines</a></span></li><li><span><a href=\"#Classes-de-voyageur\" data-toc-modified-id=\"Classes-de-voyageur-2.7\">Classes de voyageur</a></span></li><li><span><a href=\"#Ticket\" data-toc-modified-id=\"Ticket-2.8\">Ticket</a></span></li><li><span><a href=\"#Familles\" data-toc-modified-id=\"Familles-2.9\">Familles</a></span></li></ul></li><li><span><a href=\"#Train-Test-split\" data-toc-modified-id=\"Train-Test-split-3\">Train Test split</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "- https://www.kaggle.com/viveksrinivasan/analyzing-titanic-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:06:04.072145Z",
     "start_time": "2019-02-04T17:06:03.426668Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:06:04.118654Z",
     "start_time": "2019-02-04T17:06:04.075365Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_combined_data():\n",
    "    train = pd.read_csv('train.csv')\n",
    "    test = pd.read_csv('test.csv')\n",
    "\n",
    "    targets = train.Survived\n",
    "    train.drop('Survived',1,inplace=True)\n",
    "    \n",
    "    combined = train.append(test)\n",
    "    combined.reset_index(inplace=True)\n",
    "    combined.drop('index',inplace=True,axis=1)\n",
    "    \n",
    "    return combined\n",
    "\n",
    "combined = get_combined_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:06:04.138602Z",
     "start_time": "2019-02-04T17:06:04.120957Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:06:04.219192Z",
     "start_time": "2019-02-04T17:06:04.142119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of male who survived :\n",
      "0    0.811092\n",
      "1    0.188908\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Proportion of male who survived :\")\n",
    "print(train[\"Survived\"][train[\"Sex\"] == \"male\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:06:04.233960Z",
     "start_time": "2019-02-04T17:06:04.222336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of female who survived :\n",
      "1    0.742038\n",
      "0    0.257962\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Proportion of female who survived :\")\n",
    "print(train[\"Survived\"][train[\"Sex\"] == \"female\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:06:04.261139Z",
     "start_time": "2019-02-04T17:06:04.239063Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_titles():\n",
    "\n",
    "    global combined\n",
    "    \n",
    "    # we extract the title from each name\n",
    "    combined['Title'] = combined['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n",
    "    \n",
    "    # a map of more aggregated titles\n",
    "    Title_Dictionary = {\n",
    "        \"Capt\":\"Officer\", \"Col\":\"Officer\",\"Major\":\"Officer\",\"Jonkheer\":\"Royalty\",\n",
    "        \"Don\":\"Royalty\",\"Sir\" :\"Royalty\",\"Dr\":\"Officer\",\n",
    "        \"Rev\":\"Officer\",\"the Countess\":\"Royalty\",\"Dona\":\"Royalty\",\n",
    "        \"Mme\":\"Mrs\",\"Mlle\":\"Miss\",\"Ms\":\"Mrs\",\"Mr\" :\"Mr\",\"Mrs\" :\"Mrs\",\n",
    "        \"Miss\" :\"Miss\",\"Master\" :\"Master\",\"Lady\" :\"Royalty\"\n",
    "}\n",
    "    \n",
    "    # we map each title\n",
    "    combined['Title'] = combined.Title.map(Title_Dictionary)\n",
    "    \n",
    "get_titles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remplir les valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:06:04.298750Z",
     "start_time": "2019-02-04T17:06:04.263707Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined[\"Age\"] = combined.groupby(['Sex','Pclass','Title'])['Age'].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement des noms et des titres\n",
    "On passe les titres en one hot encoding et on supprime les noms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:06:04.329371Z",
     "start_time": "2019-02-04T17:06:04.301605Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_names():\n",
    "    \n",
    "    global combined\n",
    "    # we clean the Name variable\n",
    "    combined.drop('Name',axis=1,inplace=True)\n",
    "    \n",
    "    # encoding in dummy variable\n",
    "    titles_dummies = pd.get_dummies(combined['Title'],prefix='Title')\n",
    "    combined = pd.concat([combined,titles_dummies],axis=1)\n",
    "    \n",
    "    # removing the title variable\n",
    "    combined.drop('Title',axis=1,inplace=True)\n",
    "    \n",
    "process_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les catégories de prix\n",
    "Remplacement de la valeur manquante pour Fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:06:04.342456Z",
     "start_time": "2019-02-04T17:06:04.333228Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_fares():\n",
    "    \n",
    "    global combined\n",
    "    combined.Fare.fillna(combined.Fare.mean(),inplace=True)\n",
    "    \n",
    "process_fares()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point d'embarquement\n",
    "- Remplir valeurs manquantes avec la plus fréquente\n",
    "- One Hot Encoding des points d'embarquement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:06:04.361766Z",
     "start_time": "2019-02-04T17:06:04.346196Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_embarked():\n",
    "    \n",
    "    global combined\n",
    "    combined.Embarked.fillna('S',inplace=True)\n",
    "    \n",
    "    # dummy encoding \n",
    "    embarked_dummies = pd.get_dummies(combined['Embarked'],prefix='Embarked')\n",
    "    combined = pd.concat([combined,embarked_dummies],axis=1)\n",
    "    combined.drop('Embarked',axis=1,inplace=True)\n",
    "\n",
    "process_embarked()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cabines\n",
    "- Valeurs manquantes\n",
    "- Dummy Encoding\n",
    "- Suppression de 'Cabin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:06:04.395645Z",
     "start_time": "2019-02-04T17:06:04.363908Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_cabin():\n",
    "    \n",
    "    global combined\n",
    "    \n",
    "    # replacing missing cabins with U (for Unknown)\n",
    "    combined.Cabin.fillna('U',inplace=True)\n",
    "    \n",
    "    # mapping each Cabin value with the cabin letter\n",
    "    combined['Cabin'] = combined['Cabin'].map(lambda c : c[0])\n",
    "    \n",
    "    # dummy encoding ...\n",
    "    cabin_dummies = pd.get_dummies(combined['Cabin'],prefix='Cabin')\n",
    "    \n",
    "    combined = pd.concat([combined,cabin_dummies],axis=1)\n",
    "    \n",
    "    combined.drop('Cabin',axis=1,inplace=True)\n",
    "\n",
    "process_cabin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:06:04.409985Z",
     "start_time": "2019-02-04T17:06:04.399368Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_gender():\n",
    "    \n",
    "    global combined\n",
    "    combined['Sex'] = combined['Sex'].map({'male':1,'female':0})\n",
    "    \n",
    "process_gender()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes de voyageur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:06:04.436009Z",
     "start_time": "2019-02-04T17:06:04.415877Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_pclass():\n",
    "    \n",
    "    global combined\n",
    "    # encoding into 3 categories:\n",
    "    pclass_dummies = pd.get_dummies(combined['Pclass'],prefix=\"Pclass\")\n",
    "    \n",
    "    # adding dummy variables\n",
    "    combined = pd.concat([combined,pclass_dummies],axis=1)\n",
    "    \n",
    "    # removing \"Pclass\"\n",
    "    combined.drop('Pclass',axis=1,inplace=True)\n",
    "    \n",
    "\n",
    "process_pclass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:06:04.483637Z",
     "start_time": "2019-02-04T17:06:04.439162Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_ticket():\n",
    "    \n",
    "    global combined\n",
    "    \n",
    "    # a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "    def cleanTicket(ticket):\n",
    "        ticket = ticket.replace('.','')\n",
    "        ticket = ticket.replace('/','')\n",
    "        ticket = ticket.split()\n",
    "        ticket = map(lambda t : t.strip() , ticket)\n",
    "        ticket = list(filter(lambda t : not t.isdigit(), ticket))\n",
    "        if len(ticket) > 0:\n",
    "            return ticket[0]\n",
    "        else: \n",
    "            return 'XXX'\n",
    "        \n",
    "    # Extracting dummy variables from tickets:\n",
    "\n",
    "    combined['Ticket'] = combined['Ticket'].map(cleanTicket)\n",
    "    tickets_dummies = pd.get_dummies(combined['Ticket'],prefix='Ticket')\n",
    "    combined = pd.concat([combined, tickets_dummies],axis=1)\n",
    "    combined.drop('Ticket',inplace=True,axis=1)\n",
    "\n",
    "ticket  = process_ticket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Familles\n",
    "\n",
    "Création de variables pour la famille: \n",
    "- Taille de la famille.\n",
    "- One Hot vecteur pour la taille de la famille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:06:04.509439Z",
     "start_time": "2019-02-04T17:06:04.486435Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_family():\n",
    "    \n",
    "    global combined\n",
    "    # introducing a new feature : the size of families (including the passenger)\n",
    "    combined['FamilySize'] = combined['Parch'] + combined['SibSp'] + 1\n",
    "    \n",
    "    # introducing other features based on the family size\n",
    "    combined['Single'] = combined['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n",
    "    combined['SmallF'] = combined['FamilySize'].map(lambda s: 1 if  s == 2  else 0)\n",
    "    combined['MedF'] = combined['FamilySize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n",
    "    combined['LargeF'] = combined['FamilySize'].map(lambda s: 1 if s >= 5 else 0)\n",
    "    \n",
    "process_family()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:06:04.536853Z",
     "start_time": "2019-02-04T17:06:04.513648Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recover_train_test_target():\n",
    "    global combined\n",
    "    \n",
    "    train0 = pd.read_csv('train.csv')\n",
    "    \n",
    "    targets = train0.Survived\n",
    "    train = combined.loc[0:890]\n",
    "    test = combined.loc[891:]\n",
    "    \n",
    "    return train, test, targets\n",
    "\n",
    "xtrain_fe, xtest_fe, labels = recover_train_test_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:06:04.666751Z",
     "start_time": "2019-02-04T17:06:04.539589Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xtest_fe.to_csv(\"xtest_fe\", encoding='utf-8', index=False)\n",
    "xtrain_fe.to_csv(\"xtrain_fe\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:06:04.676313Z",
     "start_time": "2019-02-04T17:06:04.669225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 69)\n",
      "(418, 69)\n",
      "(891,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain_fe.shape)\n",
    "print(xtest_fe.shape)\n",
    "print(labels.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
